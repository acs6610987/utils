{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to implement the \"stochastic\" part, because tensorflow doesn't provide this level of detail. \"stochastic\" can be either one-sample-based, or minibatch-based. Namely, $dW$ in the following is calculated for one sample, or for one batch of samples.\n",
    "\n",
    "$$W = W - \\alpha \\cdot dW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (learning_rate, use_locking=False, name='GradientDescent')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.GradientDescentOptimizer\n",
    "inspect.signature(tf.train.GradientDescentOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "v = \\beta v + (1 - \\beta)dW \\qquad \\text{(velocity, averaging over previous gradients)} \\\\\n",
    "W = W - \\alpha \\cdot v\n",
    "\\end{cases}\n",
    "$$\n",
    "Hyperparameters: $\\alpha, \\beta$. \n",
    "\n",
    "$\\beta = 0.9$ is recommended by the literature, and normally practitioners don't tune this parameter. We only need to fine-tune the best learning rate $\\alpha$. By the way, $\\beta = 0.9$ essentially means we average over the previous 10 gradients, because earlier gradients have too small weights, and thus it is equivalent to a **sliding window** size of 10.\n",
    "\n",
    "Two more side notes. First, sometimes we see \n",
    "$$v = \\beta v + dW$$\n",
    "which is equivalent to the above (BUT with the same $\\beta = 0.9$, the best learning rate $\\alpha$ would be different.)\n",
    "\n",
    "Second, sometimes after computing $v$, people also apply bias correction\n",
    "$$v = \\frac{v}{1-\\beta^t}$$\n",
    "But in most practices, we don't do this because after $t>10$, the denominator approaches 1, so it doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.MomentumOptimizer\n",
    "inspect.signature(tf.train.MomentumOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "s = s + (dW)^2 \\\\\n",
    "W = W - \\frac{\\alpha \\cdot dW}{\\sqrt{s} + \\epsilon}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\epsilon = 10^{-8}$ is normally used to avoid dividing by tiny $s$.\n",
    "\n",
    "This makes the learning rate ($\\frac{\\alpha}{\\sqrt{s}}$) keeps decreasing, which I don't like because after a certain point, the learning might stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (learning_rate, initial_accumulator_value=0.1, use_locking=False, name='Adagrad')>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.AdagradOptimizer\n",
    "inspect.signature(tf.train.AdagradOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp (Root Mean Square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a similar idea from AdaGrad that we should scale the learning rate, we calculate $s$ by exponentially decaying average.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "s = \\beta s + (1-\\beta)(dW)^2 \\\\\n",
    "W = W - \\frac{\\alpha \\cdot dW}{\\sqrt{s} + \\epsilon}\n",
    "\\end{cases}\n",
    "$$\n",
    "Hyperparameters: $\\alpha$, $\\beta$.\n",
    "\n",
    "NOTE that $dW$ is a vector of params, and the above is element-wise computation, hence the scaled learning rate ($\\frac{\\alpha}{\\sqrt{s}}$) is different for each param direction. The motivation of RMSProp is to avoid the problem of oscillation for param updates when gradients are big but in two almost opposite directions between two iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, centered=False, name='RMSProp')>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.RMSPropOptimizer\n",
    "inspect.signature(tf.train.RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is also the 'momentum' part (although default 0) in RMSProp of tensorflow. In some literature, people also add momentum to RMSProp, which is then very similar to Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam (Adaptive moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the ideas of momentum and RMSProp. And we also do bias correction.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "v = \\beta_1 v + (1 - \\beta_1)dW \\\\\n",
    "s = \\beta_2 s + (1 - \\beta_2)(dW)^2 \\\\\n",
    "v = \\frac{v}{1 - \\beta_1^t} \\\\\n",
    "s = \\frac{s}{1 - \\beta_2^t} \\\\\n",
    "W = W - \\frac{\\alpha \\cdot v}{\\sqrt{s} + \\epsilon}\n",
    "\\end{cases}\n",
    "$$\n",
    "Hyperparameters: $\\alpha, \\beta_1, \\beta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.AdamOptimizer\n",
    "inspect.signature(tf.train.AdamOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaDelta is similar to RMSProp, but it also introduces an additional way to adapt the learning rate.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "s = \\beta s + (1-\\beta)(dW)^2 \\qquad \\text{(Accumulate gradients, RMSProp-like step)}\\\\\n",
    "v = -\\frac{\\sqrt{x+\\epsilon}\\cdot dW}{\\sqrt{g + \\epsilon}} \\\\\n",
    "x = \\beta x + (1-\\beta)v^2 \\qquad \\text{(Accumulate updates, momentum-like step, but why? I don't get the previous formula)}\\\\\n",
    "W = W + v\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name='Adadelta')>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.AdadeltaOptimizer\n",
    "inspect.signature(tf.train.AdadeltaOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we don't need the learning rate param, but different people give different formulas of AdaDelta online, and some of them multiply $\\alpha$ in the second formula above. So, maybe Google adopts that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accoding to Andrew NG, RMSProp and Adam work well in most scenarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
